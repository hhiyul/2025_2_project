from __future__ import annotations

import io
import json
import os
from pathlib import Path
from threading import Lock
from typing import Tuple
from fastapi.security.api_key import APIKeyHeader

import torch
import torch.nn.functional as F
from fastapi import FastAPI, File, HTTPException, UploadFile, Depends, Request, Security
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from PIL import Image, UnidentifiedImageError

from fastapi.security import APIKeyHeader, APIKeyQuery
from starlette.status import HTTP_401_UNAUTHORIZED
from .models import VAL_TRANSFORM, load_cmt_model

LABEL_KR = {
    "freshapples": "Fresh Apples",
    "freshbanana": "Fresh Banana",
    "freshcapsicum": "Fresh Capsicum",
    "freshcucumber": "Fresh Cucumber",
    "freshoranges": "Fresh Oranges",
    "freshpotato": "Fresh Potato",
    "freshtomato": "Fresh Tomato",

    "rottenapples": "Rotten Apples",
    "rottenbanana": "Rotten Banana",
    "rottencapsicum": "Rotten Capsicum",
    "rottencucumber": "Rotten Cucumber",
    "rottenoranges": "Rotten Oranges",
    "rottenpotato": "Rotten Potato",
    "rottentomato": "Rotten Tomato"
}

BASE_DIR = Path(__file__).resolve().parent

def _path_from_env_or_default(env_var: str, *relative: str) -> Path:
    v = os.getenv(env_var)
    if v:
        p = Path(v)
        if not p.is_absolute():
            p = BASE_DIR / p
        return p.resolve()
    return (BASE_DIR.joinpath(*relative)).resolve()

MODEL_PATH  = _path_from_env_or_default("MODEL_PATH",  "model_pt", "best_model_fold1.pt")
LABELS_PATH = _path_from_env_or_default("LABELS_PATH", "model_pt", "label_names.json")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


if not MODEL_PATH.exists():
    raise FileNotFoundError(f"Model checkpoint not found: {MODEL_PATH}")
if not LABELS_PATH.exists():
    raise FileNotFoundError(f"Label file not found: {LABELS_PATH}")


class InferenceResponse(BaseModel):
    filename: str
    content_type: str | None
    size_bytes: int
    prediction: str
    confidence: float



# ModelService
# ----------------------------
class ModelService:
    def __init__(self, model_path: Path, labels_path: Path) -> None:
        self.model_path = model_path
        self.labels_path = labels_path
        self.device = DEVICE
        self.transform = VAL_TRANSFORM
        self._model: torch.nn.Module | None = None
        self._labels: list[str] = []
        self._lock = Lock()

    def ensure_loaded(self) -> None:
        if self._model is None:
            with self._lock:
                if self._model is None:
                    self._load()

    def _load(self) -> None:
        if not self.model_path.exists():
            raise FileNotFoundError(f"Model checkpoint not found: {self.model_path}")
        if not self.labels_path.exists():
            raise FileNotFoundError(f"Label file not found: {self.labels_path}")

        with self.labels_path.open("r", encoding="utf-8") as f:
            labels = json.load(f)
        if not isinstance(labels, list) or not all(isinstance(s, str) for s in labels) or not labels:
            raise ValueError("Label file must be a non-empty JSON array of strings")

        model, missing, unexpected = load_cmt_model(
            model_path=self.model_path, num_classes=len(labels), device=self.device
        )
        if missing:
            print("[load] missing keys:", list(missing))
        if unexpected:
            print("[load] unexpected keys:", list(unexpected))

        self._model = model.to(self.device).eval()
        self._labels = list(labels)
        torch.set_num_threads(1)  # CPU ì„œë²„ë©´ ê³¼í•œ ìŠ¤ë ˆë“œ ë°©ì§€(ìƒí™© ë§ê²Œ ì¡°ì ˆ)

    def predict(self, image_bytes: bytes) -> Tuple[str, float]:
        self.ensure_loaded()
        assert self._model is not None
        assert self._labels

        try:
            pil = Image.open(io.BytesIO(image_bytes)).convert("RGB")
        except (UnidentifiedImageError, OSError) as exc:
            raise HTTPException(status_code=400, detail="Uploaded file is not a valid image") from exc

        x = self.transform(pil).unsqueeze(0).to(self.device)
        with torch.inference_mode():
            logits = self._model(x)
            probs = F.softmax(logits, dim=1).squeeze(0)
            conf, idx = probs.max(dim=0)
        label = self._labels[int(idx)]
        return label, float(conf)


# ----------------------------
# FastAPI ì•± & ë¼ìš°íŠ¸
# ----------------------------
app = FastAPI(
    title="ìœµì†Œí”„",
    description="ë”¥ëŸ¬ë‹ì´ì—ìš”"
    )

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"],
)

model_service = ModelService(MODEL_PATH, LABELS_PATH)

#APIí‚¤
API_KEY = "fuck-key-123"
API_KEY_NAME = "X-API-Key"
api_key_header = APIKeyHeader(name="X-API-Key", auto_error=False)

#apií‚¤ ë§ë‚˜ ì²´í¬í•˜ëŠ”ê±° í‹€ë¦¬ë©´ ì˜¤ë¥˜ì½”ë“œ ë°˜í™˜
async def check_api_key(api_key: str = Depends(api_key_header)):
    if api_key != API_KEY:
        raise HTTPException(status_code=403, detail="Invalid or missing API Key")
    return api_key

@app.get("/", summary="í™ˆ")
def root():
    return {"message": "ì •ìƒì‘ë™ ì¤‘"}


@app.get("/health", summary="ì—°ê²°ìƒíƒœí™•ì¸", dependencies=[Depends(check_api_key)])
def health():
    """
    api ì—°ê²°ìƒíƒœ ì •ìƒì¸ì§€ í™•ì¸í•˜ëŠ” ê¸°ëŠ¥
    """
    return {
        "model_loaded": model_service._model is not None,
        "labels_loaded": bool(model_service._labels),
        "device": str(DEVICE),
        "model_path": str(MODEL_PATH),
        "labels_path": str(LABELS_PATH),
    }


# âœ… ì‹œê°í…ŒìŠ¤íŠ¸ìš© UI ë‹¤ ë§Œë“¤ë©´ ì—†ì•¨ê±°ì„
@app.get("/ui", summary="ì‹œê°ìš©ui", response_class=HTMLResponse)
def ui():
    """
    í…ŒìŠ¤íŠ¸ìš© ë’¤ì—ì„œ ëŒì•„ê°€ëŠ”ì§€ ì‹œê°í™”í•¨
    """
    return HTMLResponse(
        """
<!doctype html>
<html lang="ko">
<head>
  <meta charset="utf-8"/>
  <title>Inference UI</title>
  <style>
    body { font-family: system-ui, -apple-system, sans-serif; margin: 30px; }
    .card { max-width: 520px; padding: 20px; border: 1px solid #ddd; border-radius: 12px; }
    .row { margin-top: 12px; }
    img { max-width: 100%; border-radius: 8px; }
    button { padding: 10px 14px; border-radius: 8px; border: 1px solid #ccc; cursor:pointer; }
    #result { margin-top: 10px; font-weight: 600; }
    #err { color: #b00020; margin-top: 8px; }
  </style>
</head>
<body>
  <h2>ì´ë¯¸ì§€ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸</h2>
  <div class="card">
    <div class="row">
      <input id="file" type="file" accept="image/*"/>
    </div>
    <div class="row">
      <img id="preview" alt="preview" />
    </div>
    <div class="row">
      <button id="btn">ë¶„ë¥˜ ìš”ì²­</button>
    </div>
    <div id="result"></div>
    <div id="err"></div>
  </div>

<script>
const $ = id => document.getElementById(id);

// íŒŒì¼ ì„ íƒ ì‹œ ë¯¸ë¦¬ë³´ê¸°
$("file").addEventListener("change", (e) => {
  const f = e.target.files[0];
  if (!f) return;
  const url = URL.createObjectURL(f);
  $("preview").src = url;
});

// ë¶„ë¥˜ ìš”ì²­ ë²„íŠ¼ í´ë¦­
$("btn").addEventListener("click", async () => {
  $("result").textContent = "";
  $("err").textContent = "";

  const f = $("file").files[0];
  if (!f) {
    $("err").textContent = "ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”.";
    return;
  }

  const form = new FormData();
  form.append("file", f);

  try {
    const res = await fetch("/infer", {
      method: "POST",
      headers: {
        "X-API-Key": "fuck-key-123",   // ğŸ”‘ FastAPIì—ì„œ ê²€ì‚¬í•˜ëŠ” í—¤ë”
      },
      body: form,
    });

    if (!res.ok) {
      const msg = await res.text();
      $("err").textContent = "ì˜¤ë¥˜: " + msg;
      return;
    }

    const data = await res.json();
    $("result").textContent =
      `ì˜ˆì¸¡: ${data.prediction}  (conf: ${(data.confidence * 100).toFixed(1)}%)`;

  } catch (err) {
    $("err").textContent = "ìš”ì²­ ì‹¤íŒ¨: " + err;
  }
});
</script>
</body>
</html>
        """.strip()
    )

@app.post("/infer", summary="ë”¥ëŸ¬ë‹ì¶”ë¡ ", response_model=InferenceResponse, dependencies=[Depends(check_api_key)])
async def infer(file: UploadFile = File(...)):
    """
    ë”¥ëŸ¬ë‹ ì¶”ë¡  api
    """
    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Uploaded file is empty")
    pred, conf = model_service.predict(blob)
    pred_kr = LABEL_KR.get(pred, pred)
    return InferenceResponse(
        filename=file.filename or "uploaded_image",
        content_type=file.content_type,
        size_bytes=len(blob),
        prediction=pred_kr,
        confidence=conf,
    )
